{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker Tensorflow Example\n",
    "\n",
    "## Goal: To make a recommendation engine from a set of 69 products that were purchased. Once this model is created, we will serve it via an endpoint on AWS for use in other applications.\n",
    "\n",
    "* Input data: A tensor of shape (69,1) that has a 1, 0 indicator of the respective product purchased. The data was preprocessed to hide one random product purchased for prediction (X). The label/Y data has the complete purchase set, with the one hidden product revealed\n",
    "\n",
    "* The input data is split into a train, dev and test split randomly. This only used train for training the algorithm, and dev to validate the statistics\n",
    "\n",
    "`dev shapes X: (10000, 69), Y: (10000, 69)\n",
    "test shapes X: (10000, 69), Y: (10000, 69)\n",
    "train shapes X: (113696, 69), Y: (113696, 69)`\n",
    "\n",
    "Example row of X data:\n",
    "\n",
    "`[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0]]`\n",
    "\n",
    "Example row of Y data:\n",
    "\n",
    "`[[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0]]`\n",
    "\n",
    "* Output data: A tensor of shape (69,1) with the probabilities of the products purchased. For practical sake, the recommendations should ignore the input purchases, and round the probabilities for binary predictions\n",
    "\n",
    "Example row of output:\n",
    "\n",
    "`{u'outputs': {u'products': {u'dtype': u'DT_FLOAT',\n",
    "   u'floatVal': [0.0009171566343866289,\n",
    "    0.002417637500911951,\n",
    "    0.021878276020288467,...`\n",
    "    \n",
    "    \n",
    "## First step below \n",
    "\n",
    "* import libraries for sagemaker\n",
    "* specify s3 bucket that contains the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import os\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "#IAM execution role that gives SageMaker access to resources in your AWS account.\n",
    "\n",
    "data_folder = 's3://wmp-machinelearning-poc-december2017/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source code overview\n",
    "\n",
    "## Sagemaker uses a high-level API to access tensorflow. The code is in the same directory as the notebook, and has implementations of:\n",
    "\n",
    "* model_fn - specifies the tensorflow estimator, giving details of the nn architecture, loss function, and evaluation specs\n",
    "\n",
    "* eval, serve and train input_fn - specifies how the model ingests the data for training as well as the input for when the model is served"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\r\n",
      "import os\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow.python.estimator.export.export import build_raw_serving_input_receiver_fn\r\n",
      "from tensorflow.python.estimator.export.export_output import PredictOutput\r\n",
      "\r\n",
      "INPUT_TENSOR_NAME = \"inputs\"\r\n",
      "SIGNATURE_NAME = \"serving_default\"\r\n",
      "LEARNING_RATE = 0.001\r\n",
      "\r\n",
      "\r\n",
      "def model_fn(features, labels, mode, params):\r\n",
      "    # Connect the first hidden layer to input layer\r\n",
      "    # (features[\"x\"]) with relu activation\r\n",
      "    \r\n",
      "    layer1 = tf.layers.dense(tf.cast(features[INPUT_TENSOR_NAME], tf.float32), 2048, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(seed = 1))\r\n",
      "    layer2 = tf.layers.dense(layer1, 1024, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(seed = 1))\r\n",
      "    layer3 = tf.layers.dense(layer2, 512, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(seed = 1))\r\n",
      "    layer4 = tf.layers.dense(layer3, 256, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(seed = 1))\r\n",
      "    layer5 = tf.layers.dense(layer4, 128, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(seed = 1))\r\n",
      "    logits = tf.layers.dense(layer5, 69, kernel_initializer=tf.contrib.layers.xavier_initializer(seed = 1))\r\n",
      "        # Reshape output layer to 1-dim Tensor to return predictions\r\n",
      "    predictions = tf.nn.sigmoid(logits)\r\n",
      "    rounded_predictions = tf.round(predictions)\r\n",
      "\r\n",
      "    # Provide an estimator spec for `ModeKeys.PREDICT`.\r\n",
      "    if mode == tf.estimator.ModeKeys.PREDICT:\r\n",
      "        return tf.estimator.EstimatorSpec(\r\n",
      "            mode=mode,\r\n",
      "            predictions={\"products\": predictions},\r\n",
      "            export_outputs={SIGNATURE_NAME: PredictOutput({\"products\": predictions})})\r\n",
      "\r\n",
      "    # Calculate loss using custom loss function\r\n",
      "    \r\n",
      "    hidden_products = tf.subtract(tf.cast(labels, tf.float32),\r\n",
      "                                  tf.cast(features[INPUT_TENSOR_NAME],\r\n",
      "                                          tf.float32)) #now have a vector with just ones for hidden products\r\n",
      "\r\n",
      "    hidden_mask = tf.equal(hidden_products,1)\r\n",
      "\r\n",
      "    non_hidden_mask = tf.equal(hidden_products,0)\r\n",
      "    \r\n",
      "    logit_one_only = tf.boolean_mask(logits, hidden_mask)\r\n",
      "\r\n",
      "    label_one_only = tf.boolean_mask(tf.cast(labels, tf.float32), hidden_mask)\r\n",
      "\r\n",
      "    ones_cost = tf.reduce_sum(tf.multiply(tf.constant(69.0 ** 0.5, dtype='float32'), tf.nn.sigmoid_cross_entropy_with_logits(labels=label_one_only,\r\n",
      "                                                       logits=logit_one_only)))\r\n",
      "\r\n",
      "\r\n",
      "    logit_zero_only = tf.boolean_mask(logits, non_hidden_mask)\r\n",
      "\r\n",
      "    label_zero_only = tf.boolean_mask(tf.cast(labels, tf.float32), non_hidden_mask)\r\n",
      "\r\n",
      "    zeros_cost = tf.reduce_sum(tf.multiply(tf.constant(1.0, dtype='float32'),tf.nn.sigmoid_cross_entropy_with_logits(labels=label_zero_only,\r\n",
      "                                                       logits=logit_zero_only)))\r\n",
      "\r\n",
      "    loss = tf.add(ones_cost, zeros_cost)\r\n",
      "    \r\n",
      "    optimizer = tf.train.AdamOptimizer(\r\n",
      "        learning_rate=params[\"learning_rate\"])\r\n",
      "    train_op = optimizer.minimize(\r\n",
      "        loss=loss, global_step=tf.train.get_global_step())\r\n",
      "    \r\n",
      "    ###other eval metrics\r\n",
      "    y_hat = tf.nn.sigmoid(logits)\r\n",
      "    correct_prediction = tf.equal(tf.round(y_hat), hidden_products)\r\n",
      "\r\n",
      "    true_positive_rate = tf.reduce_mean(tf.round(tf.boolean_mask(y_hat,hidden_mask)))\r\n",
      "\r\n",
      "    #false_positive_rate = tf.subtract(1.0, true_positive_rate)\r\n",
      "\r\n",
      "    false_negative_rate = tf.reduce_mean(tf.round(tf.boolean_mask(y_hat,non_hidden_mask)))\r\n",
      "\r\n",
      "    #true_negative_rate = tf.subtract(1.0, false_negative_rate)\r\n",
      "    #recall = tf.reduce_sum(tf.cast(correct_prediction, \"float\")) / tf.reduce_sum(hidden_products)\r\n",
      "\r\n",
      "    # Calculate accuracy on the test set\r\n",
      "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\r\n",
      "\r\n",
      "    # Calculate root mean squared error as additional eval metric\r\n",
      "    eval_metric_ops = {\r\n",
      "        \"custom_loss\" : tf.metrics.mean(loss),\r\n",
      "        \"accuracy\" : tf.metrics.mean(accuracy),\r\n",
      "        \"tpr\" : tf.metrics.mean(true_positive_rate),\r\n",
      "        \"fnr\" : tf.metrics.mean(false_negative_rate)\r\n",
      "    }\r\n",
      "\r\n",
      "    # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\r\n",
      "    return tf.estimator.EstimatorSpec(\r\n",
      "        mode=mode,\r\n",
      "        loss=loss,\r\n",
      "        train_op=train_op,\r\n",
      "        eval_metric_ops=eval_metric_ops)\r\n",
      "\r\n",
      "\r\n",
      "def serving_input_fn(params):\r\n",
      "    tensor = tf.placeholder(tf.float32, shape=[1, 69])\r\n",
      "    return build_raw_serving_input_receiver_fn({INPUT_TENSOR_NAME: tensor})()\r\n",
      "\r\n",
      "\r\n",
      "params = {\"learning_rate\": LEARNING_RATE}\r\n",
      "\r\n",
      "\r\n",
      "def train_input_fn(training_dir, params):\r\n",
      "    xFN = os.path.join(training_dir, 'trainX')\r\n",
      "    yFN = os.path.join(training_dir, 'trainY')\r\n",
      "    tX = np.genfromtxt(xFN, delimiter=',')\r\n",
      "    tY = np.genfromtxt(yFN, delimiter=',')\r\n",
      "    return tf.estimator.inputs.numpy_input_fn(x = {INPUT_TENSOR_NAME: np.array(tX)}, y=np.array(tY), shuffle = True, num_epochs=None)()\r\n",
      "\r\n",
      "\r\n",
      "def eval_input_fn(training_dir, params):\r\n",
      "    xFN = os.path.join(training_dir, 'devX')\r\n",
      "    yFN = os.path.join(training_dir, 'devY')\r\n",
      "    tX = np.genfromtxt(xFN, delimiter=',')\r\n",
      "    tY = np.genfromtxt(yFN, delimiter=',')\r\n",
      "    return tf.estimator.inputs.numpy_input_fn(\r\n",
      "        x={INPUT_TENSOR_NAME: np.array(tX)},\r\n",
      "        y= np.array(tY),\r\n",
      "        shuffle = True, \r\n",
      "        num_epochs=1)()\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat recommender2017.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the estimator\n",
    "\n",
    "## recEstimator is created and we specify 50 training steps.\n",
    "\n",
    "* Each training step is defined by the training input fn. Each step is a shuffled mini-batch of size 128\n",
    "* There is only one evaluation step, since we specify it to run over one epoch (entire data set) of the dev set\n",
    "\n",
    "## Fit is then called and kicks off a training job. Ideally, should do more than 50 steps, but did a bit for POC sake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-tensorflow-py2-cpu-2017-12-14-23-59-17-061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................................................\n",
      "\u001b[31mexecuting startup script (first run)\u001b[0m\n",
      "\u001b[31m2017-12-15 00:03:46,970 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2017-12-15 00:03:46,971 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2017-12-15 00:03:48,733 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2017-12-15 00:03:49,623 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2017-12-15 00:03:49,689 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.us-east-2.amazonaws.com\u001b[0m\n",
      "\u001b[31m2017-12-15 00:03:49,745 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:{\"environment\": \"cloud\", \"cluster\": {\"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"master\"}}\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:---------------------------------------------------------\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:going to training\u001b[0m\n",
      "\u001b[31m2017-12-15 00:03:49,826 INFO - root - creating RunConfig:\u001b[0m\n",
      "\u001b[31m2017-12-15 00:03:49,827 INFO - root - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[31m2017-12-15 00:03:49,827 INFO - root - creating the estimator\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Using config: {'_model_dir': u's3://sagemaker-us-east-2-185314348485/sagemaker-tensorflow-py2-cpu-2017-12-14-23-59-17-061/checkpoints', '_save_checkpoints_secs': 300, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': u'master', '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6c4ca741d0>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31m, '_num_worker_replicas': 1, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}\u001b[0m\n",
      "\u001b[31m2017-12-15 00:03:49,828 INFO - root - creating Experiment:\u001b[0m\n",
      "\u001b[31m2017-12-15 00:03:49,828 INFO - root - {'min_eval_frequency': 1000}\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:267: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mMonitors are deprecated. Please use tf.train.SessionRunHook.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[31m2017-12-15 00:04:10.358863: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving checkpoints for 1 into s3://sagemaker-us-east-2-185314348485/sagemaker-tensorflow-py2-cpu-2017-12-14-23-59-17-061/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 6758.82, step = 1\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving checkpoints for 50 into s3://sagemaker-us-east-2-185314348485/sagemaker-tensorflow-py2-cpu-2017-12-14-23-59-17-061/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Loss for final step: 1559.2.\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Starting evaluation at 2017-12-15-00:04:22\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Restoring parameters from s3://sagemaker-us-east-2-185314348485/sagemaker-tensorflow-py2-cpu-2017-12-14-23-59-17-061/checkpoints/model.ckpt-50\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [1/1]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Finished evaluation at 2017-12-15-00:04:25\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving dict for global step 50: accuracy = 0.947351, custom_loss = 1664.79, fnr = 0.0487132, global_step = 50, loss = 1664.79, tpr = 0.679688\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Restoring parameters from s3://sagemaker-us-east-2-185314348485/sagemaker-tensorflow-py2-cpu-2017-12-14-23-59-17-061/checkpoints/model.ckpt-50\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Assets added to graph.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:No assets to write.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:SavedModel written to: s3://sagemaker-us-east-2-185314348485/sagemaker-tensorflow-py2-cpu-2017-12-14-23-59-17-061/checkpoints/export/Servo/temp-1513296266/saved_model.pb\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:writing success training\u001b[0m\n",
      "\u001b[31m2017-12-15 00:04:29,410 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2017-12-15 00:04:29,469 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.us-east-2.amazonaws.com\u001b[0m\n",
      "\u001b[31m2017-12-15 00:04:29,546 INFO - tf_container.serve - Downloaded saved model at /opt/ml/model/export/Servo/1513296266/saved_model.pb\u001b[0m\n",
      "\u001b[31m2017-12-15 00:04:29,558 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): s3.us-east-2.amazonaws.com\u001b[0m\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "recEstimator = TensorFlow(entry_point='recommender2017.py',\n",
    "                               role=role,\n",
    "                               training_steps= 50,                                  \n",
    "                               evaluation_steps= 1,\n",
    "                               hyperparameters={'learning_rate': 0.001},\n",
    "                               train_instance_count=1,\n",
    "                               train_instance_type='ml.c4.xlarge')\n",
    "\n",
    "recEstimator.fit(data_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval is now complete - accuraccy is pretty good, TPR could get up to mid-high 80s if we run the model over more iterations\n",
    "\n",
    "## Here's the results on the development data set (blind to training): \n",
    "\n",
    "`INFO:tensorflow:Saving dict for global step 50: accuracy = 0.947351, custom_loss = 1664.79, fnr = 0.0487132, global_step = 50, loss = 1664.79, tpr = 0.679688`\n",
    "\n",
    "\n",
    "# the model is now trained, next step is to deploy the model to a self-contained endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-tensorflow-py2-cpu-2017-12-14-23-59-17-061\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-tensorflow-py2-cpu-2017-12-14-23-59-17-061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------------------------------!CPU times: user 576 ms, sys: 0 ns, total: 576 ms\n",
      "Wall time: 11min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "recommenderDeploy = recEstimator.deploy(initial_instance_count=1,\n",
    "                                       instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here are details for the endpoint\n",
    "\n",
    "Endpoint settings\n",
    "\n",
    "Name\n",
    "\n",
    "sagemaker-tensorflow-py2-cpu-2017-12-14-23-59-17-061\n",
    "\n",
    "ARN\n",
    "\n",
    "arn:aws:sagemaker:us-east-2:185314348485:endpoint/sagemaker-tensorflow-py2-cpu-2017-12-14-23-59-17-061\n",
    "\n",
    "Status\n",
    "\n",
    "InService\n",
    "\n",
    "Creation time\n",
    "\n",
    "Thu Dec 14 2017 18:17:58 GMT-0600 (CST)\n",
    "\n",
    "Last updated\n",
    "\n",
    "Thu Dec 14 2017 18:29:27 GMT-0600 (CST)\n",
    "\n",
    "URL\n",
    "\n",
    "https://runtime.sagemaker.us-east-2.amazonaws.com/endpoints/sagemaker-tensorflow-py2-cpu-2017-12-14-23-59-17-061/invocations\n",
    "\n",
    "Learn more about the API\n",
    "\n",
    "## below we use the built-in predict method to call the endpoint... code further down shows the python API to call predictions from other applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 98.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{u'outputs': {u'products': {u'dtype': u'DT_FLOAT',\n",
       "   u'floatVal': [0.0009171566343866289,\n",
       "    0.002417637500911951,\n",
       "    0.021878276020288467,\n",
       "    2.9551005354733206e-05,\n",
       "    5.5988140957197174e-05,\n",
       "    0.009616797789931297,\n",
       "    0.005886957049369812,\n",
       "    0.0018288253340870142,\n",
       "    0.015816371887922287,\n",
       "    0.0005179064464755356,\n",
       "    0.00011277091834926978,\n",
       "    2.7905650767934276e-07,\n",
       "    1.882578715139971e-08,\n",
       "    7.580450619570911e-05,\n",
       "    0.0007291397778317332,\n",
       "    0.007770801428705454,\n",
       "    0.007177680730819702,\n",
       "    0.0011063746642321348,\n",
       "    0.0004336817073635757,\n",
       "    6.046740963938646e-05,\n",
       "    0.002611980540677905,\n",
       "    1.9659273675642908e-05,\n",
       "    1.6322866940754466e-05,\n",
       "    0.002321285428479314,\n",
       "    0.005590972024947405,\n",
       "    0.0009698605863377452,\n",
       "    0.0063001238740980625,\n",
       "    0.0010788117069751024,\n",
       "    0.0003791811759583652,\n",
       "    0.001148053677752614,\n",
       "    0.0002344858949072659,\n",
       "    0.014281269162893295,\n",
       "    0.013773764483630657,\n",
       "    4.756671842187643e-06,\n",
       "    1.7457001376897097e-05,\n",
       "    0.006701319944113493,\n",
       "    1.0222331638942705e-06,\n",
       "    6.992815906414762e-05,\n",
       "    0.0002618895086925477,\n",
       "    0.00615578331053257,\n",
       "    0.005741918925195932,\n",
       "    0.000638678960967809,\n",
       "    0.0007860665791667998,\n",
       "    0.2672409117221832,\n",
       "    1.903335487440927e-06,\n",
       "    1.287947952732793e-06,\n",
       "    2.4009423214010894e-05,\n",
       "    4.051127575621649e-07,\n",
       "    2.6745348804979585e-05,\n",
       "    3.811816895904485e-06,\n",
       "    0.0015907511115074158,\n",
       "    0.00024060593568719923,\n",
       "    0.002875132253393531,\n",
       "    0.00012350958422757685,\n",
       "    0.00016487525135744363,\n",
       "    0.17072053253650665,\n",
       "    0.5640767216682434,\n",
       "    0.5055645108222961,\n",
       "    0.9897648096084595,\n",
       "    4.7839097533142194e-05,\n",
       "    0.19062989950180054,\n",
       "    0.0002713899884838611,\n",
       "    0.984271228313446,\n",
       "    0.10770059376955032,\n",
       "    0.055034130811691284,\n",
       "    0.7779759168624878,\n",
       "    0.7190485596656799,\n",
       "    0.003987097647041082,\n",
       "    0.0073021696880459785],\n",
       "   u'tensorShape': {u'dim': [{u'size': u'1'}, {u'size': u'69'}]}}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "recommenderDeploy.predict([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application/json'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommenderDeploy.accept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here's an endpoint call using the Python API\n",
    "\n",
    "## note that we already configured the AWS cli with the proper access keys, secrets, etc...\n",
    "\n",
    "```python\n",
    "Python 2.7.14 |Anaconda custom (64-bit)| (default, Oct  5 2017, 02:28:52) \n",
    "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>> import boto3\n",
    ">>> import json\n",
    ">>> client = boto3.client('sagemaker-runtime', region_name = 'us-east-2')\n",
    ">>> response = client.invoke_endpoint(\n",
    "...     EndpointName='sagemaker-tensorflow-py2-cpu-2017-12-14-23-59-17-061',\n",
    "...     Body=\"[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0]]\",\n",
    "...     ContentType='application/json',\n",
    "...     Accept='application/json'\n",
    "... )\n",
    ">>> j = json.loads(response['Body'].read())\n",
    ">>> j\n",
    "{u'outputs': {u'products': {u'dtype': u'DT_FLOAT', u'floatVal': [0.0009171566343866289, 0.002417637500911951, 0.021878276020288467, 2.9551005354733206e-05, 5.5988140957197174e-05, 0.009616797789931297, 0.005886957049369812, 0.0018288253340870142, 0.015816371887922287, 0.0005179064464755356, 0.00011277091834926978, 2.7905650767934276e-07, 1.882578715139971e-08, 7.580450619570911e-05, 0.0007291397778317332, 0.007770801428705454, 0.007177680730819702, 0.0011063746642321348, 0.0004336817073635757, 6.046740963938646e-05, 0.002611980540677905, 1.9659273675642908e-05, 1.6322866940754466e-05, 0.002321285428479314, 0.005590972024947405, 0.0009698605863377452, 0.0063001238740980625, 0.0010788117069751024, 0.0003791811759583652, 0.001148053677752614, 0.0002344858949072659, 0.014281269162893295, 0.013773764483630657, 4.756671842187643e-06, 1.7457001376897097e-05, 0.006701319944113493, 1.0222331638942705e-06, 6.992815906414762e-05, 0.0002618895086925477, 0.00615578331053257, 0.005741918925195932, 0.000638678960967809, 0.0007860665791667998, 0.2672409117221832, 1.903335487440927e-06, 1.287947952732793e-06, 2.4009423214010894e-05, 4.051127575621649e-07, 2.6745348804979585e-05, 3.811816895904485e-06, 0.0015907511115074158, 0.00024060593568719923, 0.002875132253393531, 0.00012350958422757685, 0.00016487525135744363, 0.17072053253650665, 0.5640767216682434, 0.5055645108222961, 0.9897648096084595, 4.7839097533142194e-05, 0.19062989950180054, 0.0002713899884838611, 0.984271228313446, 0.10770059376955032, 0.055034130811691284, 0.7779759168624878, 0.7190485596656799, 0.003987097647041082, 0.0073021696880459785], u'tensorShape': {u'dim': [{u'size': u'1'}, {u'size': u'69'}]}}}}```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
